<title>Using Random Forests to find Interesting Features on Comets</title>

This is another exercise from the <a href="https://class.coursera.org/bigdataschool-001">Caltech/JPL Summer School on Big Data Analytics</a>, specifically the one following the lectures on Decision Trees and Random Forests by Thomas Fuchs of JPL. The object of the exercise is to predict interesting features on comets and asteroids as space probes fly by them, so that onboard cameras can take detailed pictures of those spots. Manual control from the ground is not an option because time lags are significant at these distances. The exercise asks to build and train a Random Forest classifier and investigate its performance.

The dataset provided is of surface features from the comet Hartley. There are 2,089 structured observations provided as a R dataset (.rdata). The structure of the dataset is as follows:

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><div class="highlight"><pre><span class="k">struct</span> <span class="n">data</span> <span class="p">{</span>
  <span class="kt">float</span><span class="p">[</span><span class="mi">11</span><span class="p">][</span><span class="mi">11</span><span class="p">]</span> <span class="n">gray</span><span class="p">;</span>      <span class="c1">// grayscale values normalized 0..1</span>
  <span class="kt">float</span><span class="p">[</span><span class="mi">11</span><span class="p">][</span><span class="mi">11</span><span class="p">]</span> <span class="n">median</span><span class="p">;</span>    <span class="c1">// grayscale for median filtered image (0..1)</span>
  <span class="kt">int</span> <span class="n">Y</span><span class="p">;</span>                   <span class="c1">// 0 = boring; 1 = interesting</span>
  <span class="kt">int</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="n">rect</span><span class="p">;</span>             <span class="c1">// bounding box of the observation</span>
  <span class="kt">char</span><span class="o">*</span> <span class="n">frameName</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">frameId</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table>

Since I prefer to work in Python, I decided to build my classifier using the Scikit-Learn toolkit. I initially tried the <a href="https://pypi.python.org/pypi/rpy2">rpy2</a> to read this data from within Python, but it complained that the .rdata file was corrupt. So I switched around and used a combination of my rudimentary R skills and the starter code to write the data out from R into flat files, using the code below:

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><div class="highlight"><pre><span class="c1"># Source: rdata_reader.R</span>
setwd<span class="p">(</span><span class="s">&quot;/.../mlia-examples/data/hartley&quot;</span><span class="p">)</span>
dat <span class="o">&lt;-</span> dget<span class="p">(</span><span class="s">&quot;patchesHartley2.rdat&quot;</span><span class="p">)</span>

y <span class="o">&lt;-</span> sapply<span class="p">(</span>dat<span class="p">,</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> x<span class="o">$</span>Y<span class="p">)</span>
write.table<span class="p">(</span>y<span class="p">,</span> file<span class="o">=</span><span class="s">&quot;y.csv&quot;</span><span class="p">,</span> row.names<span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> col.names<span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>

Xgray <span class="o">&lt;-</span> matrix<span class="p">(</span>nrow<span class="o">=</span>length<span class="p">(</span>dat<span class="p">),</span> ncol<span class="o">=</span><span class="m">121</span><span class="p">)</span>
Xmedian <span class="o">&lt;-</span> matrix<span class="p">(</span>nrow<span class="o">=</span>length<span class="p">(</span>dat<span class="p">),</span> ncol<span class="o">=</span><span class="m">121</span><span class="p">)</span>
<span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span>length<span class="p">(</span>dat<span class="p">))</span> <span class="p">{</span>
  Xgray<span class="p">[</span>i<span class="p">,</span> <span class="p">]</span> <span class="o">=</span> as.vector<span class="p">(</span>dat<span class="p">[[</span>i<span class="p">]]</span><span class="o">$</span>gray<span class="p">)</span>
  Xmedian<span class="p">[</span>i<span class="p">,</span> <span class="p">]</span> <span class="o">=</span> as.vector<span class="p">(</span>dat<span class="p">[[</span>i<span class="p">]]</span><span class="o">$</span>median<span class="p">)</span>
<span class="p">}</span>
write.table<span class="p">(</span>Xgray<span class="p">,</span> file<span class="o">=</span><span class="s">&quot;Xgray.csv&quot;</span><span class="p">,</span> row.names<span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> col.names<span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>
write.table<span class="p">(</span>Xmedian<span class="p">,</span> file<span class="o">=</span><span class="s">&quot;Xmedian.csv&quot;</span><span class="p">,</span> row.names<span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> col.names<span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>
</pre></div>
</td></tr></table>

This creates 3 files - y.csv with 2,089 target variables one per line, and Xgray.csv and Xmedian.csv, each also with 2,089 lines and 121 (11x11) grayscale values per line.

Our Python code (shown below) builds various models with different combinations of Xgray and Xmedian, and measures their OOB (Out of Bag) error estimates. As mentioned in <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr">article</a>, in case of Random Forests, cross validation is not required to get an estimate of the test set error. The OOB is estimated internally and serves as a good estimate.

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63</pre></div></td><td class="code"><div class="highlight"><pre><span class="c"># Source: hartley_classify.py</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">DATA_DIR</span> <span class="o">=</span> <span class="s">&quot;../../data/hartley&quot;</span>

<span class="k">def</span> <span class="nf">top_n_features</span><span class="p">(</span><span class="n">fimps</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">fimps</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
                                       <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span><span class="p">])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s">&quot;y.csv&quot;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">model_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>    
    <span class="n">Xgray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s">&quot;Xgray.csv&quot;</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">)</span>
    <span class="n">Xmedian</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s">&quot;Xmedian.csv&quot;</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c"># gray values</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">Xgray</span>
    <span class="k">elif</span> <span class="n">model_id</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c"># gray values minus median values</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">Xgray</span> <span class="o">-</span> <span class="n">Xmedian</span>
    <span class="k">elif</span> <span class="n">model_id</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>        
        <span class="c"># gray values concatenated with median values</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">Xgray</span><span class="p">,</span> <span class="n">Xmedian</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">model_id</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c"># combination 1 and 2</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">Xgray</span><span class="p">,</span> <span class="n">Xmedian</span><span class="p">,</span> <span class="n">Xgray</span> <span class="o">-</span> <span class="n">Xmedian</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">model_id</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="c"># mean shift values and add them in</span>
        <span class="n">Xgray_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Xgray</span><span class="p">)</span>
        <span class="n">Xgray_ms</span> <span class="o">=</span> <span class="n">Xgray</span> <span class="o">-</span> <span class="n">Xgray_mu</span>
        <span class="n">Xmedian_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Xmedian</span><span class="p">)</span>
        <span class="n">Xmedian_ms</span> <span class="o">=</span> <span class="n">Xmedian</span> <span class="o">-</span> <span class="n">Xmedian_mu</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">Xgray</span><span class="p">,</span> <span class="n">Xmedian</span><span class="p">,</span> <span class="n">Xgray_ms</span><span class="p">,</span> <span class="n">Xmedian_ms</span><span class="p">,</span> 
                            <span class="n">Xgray</span> <span class="o">-</span> <span class="n">Xmedian</span><span class="p">,</span> <span class="n">Xgray_ms</span> <span class="o">-</span> <span class="n">Xmedian_ms</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s">&quot;auto&quot;</span><span class="p">,</span> 
                                 <span class="n">oob_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c"># Split data train/test = 75/25</span>
    <span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;OOB Score:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">oob_score_</span>
    <span class="k">print</span> <span class="s">&quot;Top 5 Features:&quot;</span><span class="p">,</span> <span class="n">top_n_features</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="c"># compute ROC curve data</span>
    <span class="n">ypred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;Model#</span><span class="si">%d</span><span class="s"> (AUC=</span><span class="si">%.2f</span><span class="s">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">))</span>

<span class="c"># baseline, axes, labels, etc</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">&quot;k--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</td></tr></table>

The first model uses Xgray values only as input variables, the second uses the difference between Xgray and Xmedian (ie the highlights), the third uses a concatenation of Xgray and Xmedian, the fourth a concatenation of Xgray, Xmedian and the difference, and the fifth uses Xgray, Xmedian, mean shifted Xgray, mean shifted Xmedian and the difference between mean shifted Xgray and mean shifted Xmedian. OOB Estimates on the full dataset were in the 95-97% range, and 90-93% on a 25% held out test set. Here are the OOB estimates for each model.

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="highlight"><pre>Model 1: 0.924648786718
Model 2: 0.907407407407
Model 3: 0.92975734355
Model 4: 0.925287356322
Model 5: 0.932950191571
</pre></div>
</td></tr></table>

From the results, Model #5 performed the best. Random Forests can also provide an ordering of their input features by importance. This information can be used to retrain the classifier with only the top N features. I haven't tried this, but it may be worth trying out to see if results improve further. The top 5 features for each model was as follows.

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="highlight"><pre>Model 1 Top 5 Features: [60, 71, 61, 49, 50]
Model 2 Top 5 Features: [60, 57, 49, 46, 71]
Model 3 Top 5 Features: [181, 170, 60, 192, 61]
Model 4 Top 5 Features: [181, 192, 170, 60, 182]
Model 5 Top 5 Features: [181, 423, 412, 192, 60]
</pre></div>
</td></tr></table>

Finally, we plot the ROC (Reciever Operating Characteristic, basically a plot of False Positive Rate against True Positive Rate) curves for each of the 5 classifiers. The dotted line represents the baseline. Using the AUC (Area under the curve), Model 1 looks slightly better than Model 5. However, all the models look almost equally good, probably because it is a toy example.

<img src="roc_chart.png"/>

Thats all I have for today. In the past, I hadn't looked at Random Forests too much because it is not that commonly used for text classification (although I suspect the reason for that may just be a historical preference for other methods).

